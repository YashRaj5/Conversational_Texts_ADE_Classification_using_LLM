{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26d7573a",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "information about this solution accelerator is given at below link\n",
    "\n",
    "https://www.databricks.com/solutions/accelerators/adverse-drug-event-detection\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c6be8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Util:\n",
    "    def __init__(self, project_name, base_path=None):\n",
    "        if base_path != None:\n",
    "            self.base_path = base_path\n",
    "        else:\n",
    "            user = \"yraj\"\n",
    "\n",
    "            # for databricks\n",
    "            # user=dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().apply('user')\n",
    "\n",
    "            self.base_path = f\"C:\\\\Users\\\\{user}\\\\Work\\\\POCs\\\\Drugs & Adverse Events\"\n",
    "\n",
    "        # self.project_name = project_name.strip().replace(' ', '-')\n",
    "        self.data_path = f\"{self.base_path}\\\\data\"\n",
    "        self.delta_path = f\"{self.base_path}\\\\delta\"\n",
    "\n",
    "        import os\n",
    "        \n",
    "        try:\n",
    "            os.listdir(f'{self.data_path}')\n",
    "            print(f'{self.data_path} is already present')\n",
    "            os.listdir(f'{self.delta_path}')\n",
    "            print(f'{self.delta_path} is already present')\n",
    "        except:\n",
    "            os.mkdir(self.data_path)\n",
    "            os.mkdir(self.delta_path)\n",
    "\n",
    "        # for Databricks\n",
    "        # dbutils.fs.mkdirs(self.base_path)\n",
    "        # dbutils.fs.mkdirs(self.data_path)\n",
    "        # dbutils.fs.mkdirs(self.delta_path)\n",
    "        \n",
    "        # for storing vector data\n",
    "        self.vector_store_path = f\"{self.data_path}\\\\vector_store\"\n",
    "        \n",
    "        # models name for registering\n",
    "        self.registered_model_name = \"ade-llm\"\n",
    "        self.embedding_model_name = \"all-MiniLM-L12-v2\"\n",
    "        self.openai_chat_model = \"gpt-3.5-turbo\"\n",
    "        self.system_message_template = \"\"\"You are a helpful assistant built by Yash, you are good at helping classification of drug and it's affect based on the context provided, the context is a document. If the context does not provide enough relevant information to determine the answer, just say I don't know. If the context is irrelevant to the question, just say I don't know. If you did not find a good answer from the context, just say I don't know. If the query doesn't form a complete question, just say I don't know. If there is a good answer from the context, try to summarize the context to answer the question.\"\"\"\n",
    "        self.human_message_template = \"\"\"Given the context: {context}. Classify the drug and it's affect {statement}.\"\"\"\n",
    "\n",
    "        # MLflow settings\n",
    "        import mlflow\n",
    "        _ = mlflow.set_experiment(f'{self.base_path}\\\\{self.registered_model_name}')\n",
    "\n",
    "    def load_remote_data(self, url, unpack=False):\n",
    "        import requests\n",
    "\n",
    "        fname = url.split(\"/\")[-1]\n",
    "        r = requests.get(url)\n",
    "        print(\"*\" * 100)\n",
    "        print(f\"downloading file {fname} to {self.data_path}\")\n",
    "        print(\"*\" * 100)\n",
    "        open(f\"{self.data_path}\\\\{fname}\", \"wb\").write(r.content)\n",
    "        if unpack:\n",
    "            import tarfile\n",
    "\n",
    "            # open file\n",
    "            file = tarfile.open(f\"{self.data_path}\\\\{fname}\")\n",
    "            file.extractall(f\"{self.data_path}\")\n",
    "            file.close()\n",
    "\n",
    "    def print_paths(self):\n",
    "        print(f\"root folder                    : {self.base_path}\")\n",
    "        print(f\"raw data location              : {self.data_path}\")\n",
    "        print(f\"delta tables location          : {self.delta_path}\")\n",
    "        print(f\"vector store location          : {self.vector_store_path}\")\n",
    "        print(f\"mlflow experitment location    : {self.base_path}\\\\{self.registered_model_name}\")\n",
    "        print(f\"model name                     : {self.registered_model_name}\")\n",
    "\n",
    "        # for Databricks\n",
    "        # html_str = f\"\"\"\n",
    "        # <p>\n",
    "        # <b>base_path</b> = <i>{self.base_path}</i><br>\n",
    "        # <b>data_path</b> [where your raw data will be stored] = <i>{self.data_path}</i><br>\n",
    "        # <b>delta_path</b> [where your delta tables will be stored] = <i>{self.delta_path}</i><br>\n",
    "        # </p>\n",
    "        # \"\"\"\n",
    "        # displayHTML(html_str)\n",
    "\n",
    "    def display_data(self):\n",
    "        import os\n",
    "\n",
    "        files = os.listdir(f\"{self.data_path}\")\n",
    "\n",
    "        # for Databricks\n",
    "        # files = dbutils.fs.ls(f'{self.data_path}')\n",
    "        if len(files) == 0:\n",
    "            print(\"no data available, please run load_remote_data(<url for the data>)\")\n",
    "        else:\n",
    "            print(\"*\" * 100)\n",
    "            print(f\"data available in {self.data_path} are:\")\n",
    "            print(\"*\" * 100)\n",
    "            for _ in files:\n",
    "                print(_)\n",
    "            # for Databricks\n",
    "            # display(files)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
